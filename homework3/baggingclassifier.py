# -*- coding: utf-8 -*-
"""BaggingClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N9LnOb_p0i81g1eRMuzACkhD4cAIamxE
"""

import csv
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import metrics

clauses = [300, 500, 1000, 1500, 1800]
samples = [100, 1000, 5000]
dataset = ['train', 'valid', 'test']
base_path = 'hw3_part1_data'

params_store = {
    '300_100': {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 200, 'max_features': 500, 'splitter': 'best'},
    '500_100': {'class_weight': None, 'criterion': 'entropy', 'max_depth': 120, 'max_features': None, 'splitter': 'best'},
    '1000_100': {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 200, 'max_features': None, 'splitter': 'best'},
    '1500_100': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'splitter': 'random'},
    '1800_100': {'class_weight': None, 'criterion': 'entropy', 'max_depth': 100, 'max_features': None, 'splitter': 'best'},

    '300_1000': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 150, 'max_features': None, 'splitter': 'best'},
    '500_1000': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 100, 'max_features': 500, 'splitter': 'best'},
    '1000_1000': {'class_weight': None, 'criterion': 'entropy', 'max_depth': 150, 'max_features': None, 'splitter': 'best'},
    '1500_1000': {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 150, 'max_features': 500, 'splitter': 'best'},
    '1800_1000': {'class_weight': None, 'criterion': 'entropy', 'max_depth': 200, 'max_features': 500, 'splitter': 'best'},

    '300_5000': {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 120, 'max_features': None, 'splitter': 'best'},
    '500_5000': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 100, 'max_features': None, 'splitter': 'best'},
    '1000_5000': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 200, 'max_features': 500, 'splitter': 'best'},
    '1500_5000': {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 200, 'max_features': 500, 'splitter': 'best'},
    '1800_5000': {'class_weight': None, 'criterion': 'entropy', 'max_depth': 200, 'max_features': 500, 'splitter': 'best'},
}

final_result = {}
for sample in samples:
    for clause in clauses:
        data = [0] * len(dataset)
        label = [0] * len(dataset)
        for i, sets in enumerate(dataset):
            x = []
            y = []
            with open(base_path + '/' + sets + '_c' + str(clause) + '_d' + str(sample) + '.csv', newline='') as csvfile:
                for row in csv.reader(csvfile):
                    x.append([int(x)for x in row[:len(row) - 1]])
                    y.append(int(row[-1]))
                data[i] = x
                label[i] = y

        name = str(clause) + " " + str(sample)
        params = params_store[str(clause)+"_"+str(sample)]
        clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion=params['criterion'], splitter=params['splitter'], max_features=params['max_features'], max_depth=params['max_depth'], class_weight=params['class_weight']),
                                n_estimators=10,
                                random_state=True, bootstrap_features=True, warm_start=True, n_jobs=-1)
        clf.fit(data[0]+data[1], label[0]+label[1])
        pred = clf.predict(data[2])
        print(name, metrics.accuracy_score(label[2], pred), metrics.f1_score(label[2], pred))

        final_result.setdefault(name, {})
        final_result[name]['params'] = params
        final_result[name]['f1_score'] = metrics.f1_score(label[2], pred)
        final_result[name]['accuracy_score'] = metrics.accuracy_score(label[2], pred)
print(final_result)
