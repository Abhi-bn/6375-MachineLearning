{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team Members\n",
        "1. AXB210119\tAbhinava Bharamasagara Nanjundaiah\n",
        "2. HXD220007\tHarsha Priya Daggubati\n",
        "3. PXP210104\tPritika Priyadarshini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcTxV9cAi6kc",
        "outputId": "da5bff6a-5df3-4a5f-f66e-6da6ca7221f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'VEC' already exists and is not an empty directory.\n",
            "/home/abhinava/Multi-Class-Bayesian/VEC\n",
            "Requirement already satisfied: igraph in /home/abhinava/.local/lib/python3.8/site-packages (0.10.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /home/abhinava/.local/lib/python3.8/site-packages (from igraph) (1.6.4)\n",
            "Requirement already satisfied: Cython in /home/abhinava/.local/lib/python3.8/site-packages (0.29.32)\n",
            "Processing /home/abhinava/Multi-Class-Bayesian/VEC\n",
            "Building wheels for collected packages: TPM\n",
            "  Building wheel for TPM (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for TPM: filename=TPM-0.0.0-cp38-cp38-linux_x86_64.whl size=2755803 sha256=450c234071af24912796f4fe3296930687c39da68eeb6b8e60aa0583ae5ad926\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nv3zjqzk/wheels/2e/d5/44/c81e6a35fc73ef0bcff6cb95c982a259c326a0c801c11cfb8e\n",
            "Successfully built TPM\n",
            "Installing collected packages: TPM\n",
            "  Attempting uninstall: TPM\n",
            "    Found existing installation: TPM 0.0.0\n",
            "    Uninstalling TPM-0.0.0:\n",
            "      Successfully uninstalled TPM-0.0.0\n",
            "Successfully installed TPM-0.0.0\n",
            "/home/abhinava/Multi-Class-Bayesian\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vkomaragiri/VEC.git\n",
        "%cd ./VEC/\n",
        "!pip install igraph\n",
        "!pip install Cython\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HalqEXd-isAg",
        "outputId": "e2a954ec-cab9-4573-cd2b-9531ee877bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from MN import MN \n",
        "from BTP import BTP\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd1xf0IxisAk"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def __init__(self, fpath):\n",
        "\n",
        "        f = open(fpath, \"r\")\n",
        "\n",
        "        self.nvars = int(f.readline())  # 1\n",
        "\n",
        "        line = np.asarray(f.readline().split(), dtype=np.int32)  # 2\n",
        "        self.evid_var_ids = line[1:]\n",
        "        evid_indices = range(1, self.evid_var_ids.shape[0]*2, 2)\n",
        "\n",
        "        line = np.asarray(f.readline().split(), dtype=np.int32)  # 3\n",
        "        self.query_var_ids = line[1:]\n",
        "        query_indices = range(\n",
        "            self.evid_var_ids.shape[0]*2+1, (self.evid_var_ids.shape[0]+self.query_var_ids.shape[0])*2, 2)\n",
        "\n",
        "        line = np.asarray(f.readline().split(), dtype=np.int32)  # 4\n",
        "        self.hidden_var_ids = line[1:]\n",
        "\n",
        "        line = f.readline()  # 5\n",
        "        self.nproblems = int(f.readline())  # 6\n",
        "\n",
        "        self.evid_assignments = []\n",
        "        self.query_assignments = []\n",
        "        self.weights = []\n",
        "        for i in range(self.nproblems):\n",
        "            line = np.asarray(f.readline().split(), dtype=float)\n",
        "            self.evid_assignments.append(np.asarray(\n",
        "                line[evid_indices], dtype=np.int32))\n",
        "            self.query_assignments.append(np.asarray(\n",
        "                line[query_indices], dtype=np.int32))\n",
        "            self.weights.append(line[-1])\n",
        "        self.evid_assignments = np.asarray(self.evid_assignments)\n",
        "        self.query_assignments = np.asarray(self.query_assignments)\n",
        "        self.weights = np.asarray(self.weights)\n",
        "        self.hidden_assignments = []\n",
        "\n",
        "    def convertToXYWithH(self, hidden_assignments):\n",
        "        return (np.concatenate((self.evid_assignments, hidden_assignments), axis=1), self.query_assignments)\n",
        "\n",
        "    def convertToXY(self):\n",
        "        return (self.evid_assignments, self.query_assignments)\n",
        "\n",
        "    def convertResults(self, query_predictions, removed_qvars):\n",
        "        self.query_var_ids = np.delete(self.query_var_ids, removed_qvars)\n",
        "        out = np.zeros(\n",
        "            (query_predictions.shape[0], 1+2*self.query_var_ids.shape[0]), dtype=int)\n",
        "        out[:, 2::2] = query_predictions[:, :]\n",
        "        out[:, 1::2] = self.query_var_ids\n",
        "        out[:, 0] = self.query_var_ids.shape[0]\n",
        "        return out\n",
        "\n",
        "    def computeLogProb(self, mn, order, X, y):\n",
        "        out = np.zeros(X.shape[0])\n",
        "        for i in range(X.shape[0]):\n",
        "            for j in range(len(self.evid_var_ids)):\n",
        "                mn.setEvidence(self.evid_var_ids[j], X[i][j])\n",
        "            for j in range(y.shape[1]):\n",
        "                mn.setEvidence(self.query_var_ids[j], y[i][j])\n",
        "            btp = BTP(mn, order)\n",
        "            out[i] = np.log10(btp.getPR())\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def computeErr(true_ll, pred_ll):\n",
        "        return np.sum(true_ll)-np.sum(pred_ll)\n",
        "\n",
        "    @staticmethod\n",
        "    def computeScore(err, max_err):\n",
        "        return np.max((0, 100*(1.0-err/max_err)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLNBuFxaisAo"
      },
      "outputs": [],
      "source": [
        "data_directory = './content/MLC/'\n",
        "dname = 'Sample_1_MLC_2022'\n",
        "data = Data(data_directory+dname+'.data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq--Q_WNisAp",
        "outputId": "967568d3-91cc-4170-c991-00ed74498b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden_assignments: \n",
            " [[0 1 1 ... 0 1 1]\n",
            " [0 1 1 ... 1 1 0]\n",
            " [1 0 0 ... 0 0 1]\n",
            " ...\n",
            " [1 1 1 ... 0 1 1]\n",
            " [1 1 0 ... 0 1 0]\n",
            " [1 0 1 ... 0 1 1]]\n",
            "X_train_hidden len:  808\n",
            "X_train_old len:  400\n"
          ]
        }
      ],
      "source": [
        "hidden_assignments = np.loadtxt(data_directory+dname+'.new_features', delimiter=' ', dtype=np.int32)\n",
        "print(\"hidden_assignments: \\n\", hidden_assignments)\n",
        "X_with_hidden, Y_with_hidden = data.convertToXYWithH(hidden_assignments)\n",
        "X_train_hidden, X_test_hidden, Y_train_hidden, Y_test_hidden = train_test_split(X_with_hidden, Y_with_hidden, test_size=0.33, shuffle=False)\n",
        "print(\"X_train_hidden len: \", len(X_train_hidden[0]))\n",
        "\n",
        "X_old, y_old = data.convertToXY()\n",
        "X_train_old, X_test_old, y_train_old, y_test_old = train_test_split(X_old, y_old, test_size=0.33, shuffle=False)\n",
        "print(\"X_train_old len: \",len(X_train_old[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# label cleaning\n",
        "col_to_remove = []\n",
        "for j in range(y_train_old.shape[1]):\n",
        "    if len(set(y_train_old[:,j])) == 1:\n",
        "        print(\"useless column as it has one class\", y_train_old[:,j])\n",
        "        print(j)\n",
        "        col_to_remove.append(j)\n",
        "Y_train_hidden = np.delete(Y_train_hidden, col_to_remove, 1)\n",
        "Y_test_hidden = np.delete(Y_test_hidden, col_to_remove, 1)\n",
        "y_train_old = np.delete(y_train_old, col_to_remove, 1)\n",
        "y_test_old = np.delete(y_test_old, col_to_remove, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67FX5BQjisAx"
      },
      "outputs": [],
      "source": [
        "base_logistic = MultiOutputClassifier(LogisticRegression(max_iter=1000, n_jobs=-1)).fit(X_train_old, y_train_old)\n",
        "base_logistic_pred = base_logistic.predict(X_test_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5An4qWohisAy"
      },
      "outputs": [],
      "source": [
        "# Not much change using NN\n",
        "# neural_network = MLPClassifier(max_iter=1000).fit(X_train_hidden, Y_train_hidden)\n",
        "# neural_network_pred = neural_network.predict(X_test_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J28WKMHisAz"
      },
      "outputs": [],
      "source": [
        "logistic = MultiOutputClassifier(LogisticRegression(max_iter=1000, n_jobs=-1)).fit(X_train_hidden, Y_train_hidden)\n",
        "logistic_pred = logistic.predict(X_test_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AQ9NbEWisA0"
      },
      "outputs": [],
      "source": [
        "random_forest = MultiOutputClassifier(RandomForestClassifier(n_estimators = 10, max_depth=2, n_jobs=-1)).fit(X_train_old, y_train_old)\n",
        "random_forest_pred = random_forest.predict(X_test_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d2wTTEwisA1"
      },
      "outputs": [],
      "source": [
        "random_forest_hidden = MultiOutputClassifier(RandomForestClassifier(n_estimators = 10, max_depth=5, n_jobs=-1)).fit(X_train_hidden, Y_train_hidden)\n",
        "random_forest_hidden_pred = random_forest_hidden.predict(X_test_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFp1rDzRVeYG"
      },
      "outputs": [],
      "source": [
        "pickle.dump(random_forest_hidden, open(data_directory+\"model/\"+dname+'.random_forest_hidden_model', 'wb'))\n",
        "pickle.dump(random_forest, open(data_directory+\"model/\"+dname+'.random_forest_base_model', 'wb'))\n",
        "pickle.dump(logistic, open(data_directory+\"model/\"+dname+'.logistic_hidden_model', 'wb'))\n",
        "pickle.dump(base_logistic, open(data_directory+\"model/\"+dname+'.logistic_basic_model', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORcrhABxisA1"
      },
      "outputs": [],
      "source": [
        "order = np.asarray(np.arange(data.nvars), dtype=np.int32)\n",
        "np.random.shuffle(order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-RU0UqBisA2"
      },
      "outputs": [],
      "source": [
        "# running for only sub-sample of test\n",
        "nTest = 10 #len(y_test_old)\n",
        "mn = MN()\n",
        "mn.read(data_directory+dname+'.uai')\n",
        "lProb_true = data.computeLogProb(mn, order, X_test_old[:nTest, :], y_test_old[:nTest, :])\n",
        "lProb_trivial = data.computeLogProb(mn, order, X_test_old[:nTest, :], random_forest_pred[:nTest, :])\n",
        "lProb_base_lr = data.computeLogProb(mn, order, X_test_old[:nTest, :], base_logistic_pred[:nTest, :])\n",
        "lProb_pred = data.computeLogProb(mn, order, X_test_hidden[:nTest, :], logistic_pred[:nTest, :])\n",
        "# lProb_nn_hidden = data.computeLogProb(mn, order, X_test_hidden[:nTest, :], random_forest_hidden_pred[:nTest, :])\n",
        "lProb_hidden_trivial = data.computeLogProb(mn, order, X_test_hidden[:nTest, :], random_forest_hidden_pred[:nTest, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXDqi4AsisA3"
      },
      "outputs": [],
      "source": [
        "hiddenLRErr = Data.computeErr(lProb_true, lProb_pred) \n",
        "maxTrivialErr = Data.computeErr(lProb_true, lProb_trivial)\n",
        "maxHiddenErr = Data.computeErr(lProb_true, lProb_hidden_trivial)\n",
        "maxBaseLRErr = Data.computeErr(lProb_true, lProb_base_lr)\n",
        "# maxNNErr = Data.computeErr(lProb_true, lProb_nn_hidden)\n",
        "\n",
        "print(\"Error with Random Forest `with` Hidden Assignments: \\t\\t\\t\", maxHiddenErr)\n",
        "print(\"Error with Random Forest `without` Hidden Assignments: \\t\\t\\t\", maxTrivialErr)\n",
        "print(\"Error with Logistic Regression `with` Hidden Assignments: \\t\\t\", hiddenLRErr)\n",
        "print(\"Error with Logistic Regression `without` Hidden Assignments: \\t\\t\", maxBaseLRErr)\n",
        "# print(\"Error with NN `with` Hidden Assignments: \\t\\t\\t\", maxNNErr)\n",
        "print()\n",
        "print(\"Score LR `without` hidden vs Random Forest `without` Hidden Assignments: \", Data.computeScore(maxBaseLRErr, maxTrivialErr))\n",
        "print(\"Score LR `with` hidden vs Random Forest `without` Hidden Assignments: \\t\", Data.computeScore(hiddenLRErr, maxTrivialErr ))\n",
        "print(\"Score LR `with` hidden vs Random Forest `with` hidden Assignments: \\t\", Data.computeScore(hiddenLRErr, maxHiddenErr))\n",
        "# print(\"Score NN `with` hidden vs Random Forest `without` hidden Assignments: \\t\", Data.computeScore(maxNNErr, maxTrivialErr))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
